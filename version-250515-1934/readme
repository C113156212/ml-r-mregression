這個專案，是用以計算該學生是否將會入取大學。

# 1. 建立模擬資料
data = {
    "score": [30, 45, 50, 55, 65, 70, 80, 85, 90, 95],
    "performance": [1, 1, 2, 2, 2, 3, 3, 3, 3, 3],
    "admitted": [1,1,1,1,0,0,0,0,0,0]#[0, 0, 0, 0, 1, 1, 1, 1, 1, 1]
}

# 7. 輸出準確率
accuracy = accuracy_score(y_test, y_pred)
print("模型準確率：", accuracy)

# 8. 顯示測試資料的預測結果
results = pd.DataFrame({
    "score": X_test["score"],
    "performance": X_test["performance"],
    "實際錄取": y_test,
    "預測錄取": y_pred
})

值得注意的地方是，這個專案是用拆分資料集的方式來進行訓練和測試的，這樣的作法存在一些問題：
資料集的拆分可能會導致模型的準確率不穩定，因為不同的拆分方式可能會導致不同的結果。
所以決定改成交叉驗證的方式來進行訓練和測試，這樣可以更好地評估模型的性能。

;//////////////////////////////

[以下是copilot生成的內容]

在實作中我發現，監督式學習的模型通常會使用交叉驗證（Cross Validation）來評估模型的表現。
這是因為在實際應用中，我們往往無法獲得足夠的數據來進行訓練和測試，
因此需要將數據分成多個子集，然後在不同的子集上進行訓練和測試，以獲得更可靠的評估結果。

--

[以下是我補充的內容：]

實作中，我認識到，監督式學習中，回歸的準確值判定十分有趣，是模型嘗試符合"正確資料"，也就是說，當我把側資反過來，例如:
正確資料應是[0, 0, 0, 0, 1, 1, 1, 1]，但我卻給了模型[1, 1, 1, 1, 0, 0, 0, 0]，那麼模型的準確率'依舊會是100%'，這是因為模型會試著去符合"正確資料"，而不是"正確答案"。

--

[以下是chatGPT生成的內容：]

你雖然把前面的標籤改成了 1，後面是 0（跟之前相反），但這對模型來說沒差，它看的是：

「分數小 + 表現差 → 類別是 1」
「分數高 + 表現好 → 類別是 0」

這邏輯一致而明確，所以模型還是能學到，而且準確預測。

🧠 更具體來說：
準確率的計算方式是這樣：
python
複製
編輯
accuracy_score(y_test, y_pred)
y_test 是你自己手動指定的「正確答案」

y_pred 是模型根據訓練學習出來後對測試資料的預測

accuracy = 預測正確的數量 / 總預測數量

模型只是在看：

「我猜的答案有多少個跟你當初說的答案一樣？」

;//////////////////////////////
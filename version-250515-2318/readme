



這個版本，我們終於秉棄了模組，而是手打出了模型。
並接觸到了 損失函數loss 、 權重更新。
有趣的是，當損失函數loss 太大或太小，都會出現問題:

;loss = 0.001 時:

第 0 次訓練 - Loss: 0.6931
第 100 次訓練 - Loss: 0.5912
第 200 次訓練 - Loss: 0.5899
第 300 次訓練 - Loss: 0.5886
第 400 次訓練 - Loss: 0.5874
第 500 次訓練 - Loss: 0.5861
第 600 次訓練 - Loss: 0.5848
第 700 次訓練 - Loss: 0.5836
第 800 次訓練 - Loss: 0.5823
第 900 次訓練 - Loss: 0.5810
訓練完成！

很明顯，有在收斂，但收斂的速度很慢。

;loss = 0.01 時:

第 0 次訓練 - Loss: 0.6931
第 100 次訓練 - Loss: 0.6085
第 200 次訓練 - Loss: 4.8834
第 300 次訓練 - Loss: 0.5812
第 400 次訓練 - Loss: 0.6449
第 500 次訓練 - Loss: 0.6402
第 600 次訓練 - Loss: 0.9054
第 700 次訓練 - Loss: 0.5307
第 800 次訓練 - Loss: 0.8542
第 900 次訓練 - Loss: 0.5180
訓練完成！

也很明顯的，發生震盪，無法成功收斂。

;總結:
;   所以應該改成交叉驗證的方式來進行訓練和測試，這樣可以更好地評估模型的性能。

-----------------

;以下是權重更新的過程:

'''
    梯度（偏導數）

        權重梯度：
            ∂𝐿 / ∂𝑤 = (1/𝑛) ⋅ 𝑋𝑇 ⋅ error

        先計算：
            X^T = [[30, 70],
                    [1, 3]]
                    
            error = [-0.044, 0.9993]

        矩陣乘法：
            dw[0] = (30 * -0.044 + 70 * 0.9993) / 2 ≈ 34.33
            dw[1] = (1 * -0.044 + 3 * 0.9993) / 2 ≈ 1.48

        所以：
            dw ≈ [34.33, 1.48]

        --

        
        截距梯度：
            ∂𝐿 / ∂𝑏 = (1/𝑛) ⋅ Σ error = (-0.044 + 0.9993)/2 ≈ 0.478

        ---

        ## 5️⃣ 更新權重與截距：

        使用學習率 學習率𝜂 = 0.01

        w_new = w - 𝜂 ⋅ (∂L/∂w) = [0.1, 0.1] - 0.01 ⋅ [34.33, 1.48] = [-0.2433, 0.0852]
        b_new = b - 𝜂 ⋅ (∂L/∂b) = 0.0 - 0.01 ⋅ 0.478 = -0.0048
        
        這樣就完成了一次的參數更新。
        這個過程會重複進行多次（epochs），每次都會計算新的權重和截距，直到模型收斂。

    '''